{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4558742,"sourceType":"datasetVersion","datasetId":2660745}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport pickle\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nimport tiktoken","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:47.495808Z","iopub.execute_input":"2025-05-19T13:29:47.495983Z","iopub.status.idle":"2025-05-19T13:29:51.065422Z","shell.execute_reply.started":"2025-05-19T13:29:47.495967Z","shell.execute_reply":"2025-05-19T13:29:51.064709Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"@dataclass\nclass GPTConfig:\n    vocab_size: int\n    block_size: int = 128\n    embed_size: int = 256\n    num_layers: int = 6\n    forward_expansion: int = 4\n    heads: int = 8\n    dropout: float = 0.05\n    batch_size: int = 64\n    max_iters: int = 10_000\n    eval_interval: int = 500\n    learning_rate: float = 1e-4\n    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n    checkpoint_path: str = 'gpt_simple.pt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:51.066271Z","iopub.execute_input":"2025-05-19T13:29:51.067029Z","iopub.status.idle":"2025-05-19T13:29:51.142713Z","shell.execute_reply.started":"2025-05-19T13:29:51.067002Z","shell.execute_reply":"2025-05-19T13:29:51.141892Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"tokenizer = tiktoken.get_encoding(\"gpt2\")\n\nencode = lambda s: tokenizer.encode(s)\ndecode = lambda t: tokenizer.decode(t)\nvocab_size = tokenizer.n_vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:51.144522Z","iopub.execute_input":"2025-05-19T13:29:51.144782Z","iopub.status.idle":"2025-05-19T13:29:54.883604Z","shell.execute_reply.started":"2025-05-19T13:29:51.144765Z","shell.execute_reply":"2025-05-19T13:29:54.882812Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"with open('/kaggle/input/the-bards-best-a-character-modeling-dataset/train.csv', 'r') as f:\n    text = f.read()\n\nprint(text[:300])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:54.884443Z","iopub.execute_input":"2025-05-19T13:29:54.884686Z","iopub.status.idle":"2025-05-19T13:29:54.914118Z","shell.execute_reply.started":"2025-05-19T13:29:54.884668Z","shell.execute_reply":"2025-05-19T13:29:54.913581Z"}},"outputs":[{"name":"stdout","text":"text\n\"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"data = torch.tensor(encode(text), dtype=torch.long)\n\nsplit = int(0.9 * len(data))\ntrain_data, val_data = data[:split], data[split:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:54.914715Z","iopub.execute_input":"2025-05-19T13:29:54.914908Z","iopub.status.idle":"2025-05-19T13:29:55.094762Z","shell.execute_reply.started":"2025-05-19T13:29:54.914892Z","shell.execute_reply":"2025-05-19T13:29:55.094218Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def get_batch(data, config):\n    ix = torch.randint(len(data) - config.block_size, (config.batch_size,))\n    x = torch.stack([data[i:i + config.block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + 1 + config.block_size] for i in ix])\n    return x.to(config.device), y.to(config.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:55.095381Z","iopub.execute_input":"2025-05-19T13:29:55.095608Z","iopub.status.idle":"2025-05-19T13:29:55.099795Z","shell.execute_reply.started":"2025-05-19T13:29:55.095591Z","shell.execute_reply":"2025-05-19T13:29:55.099111Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n    def __init__(self, embed_size, heads, dropout, max_length=512):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_size, heads, dropout=dropout, batch_first=True)\n        self.register_buffer(\"mask\", torch.tril(torch.ones(max_length, max_length)))\n\n    def forward(self, x):\n        T = x.size(1)\n        mask = self.mask[:T, :T] == 0\n        x, _ = self.attn(x, x, x, attn_mask=mask)\n        return x\n\nclass MLP(nn.Module):\n    def __init__(self, embed_size, expansion, dropout):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(embed_size, expansion * embed_size),\n            nn.GELU(),\n            nn.Linear(expansion * embed_size, embed_size),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x): \n        return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, embed_size, heads, expansion, dropout):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(embed_size)\n        self.attn = CausalSelfAttention(embed_size, heads, dropout)\n        self.ln2 = nn.LayerNorm(embed_size)\n        self.mlp = MLP(embed_size, expansion, dropout)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        self.config = config\n        self.token_embedding = nn.Embedding(config.vocab_size, config.embed_size)\n        self.position_embedding = nn.Embedding(config.block_size, config.embed_size)\n        self.blocks = nn.Sequential(*[\n            Block(config.embed_size, config.heads, config.forward_expansion, config.dropout)\n            for _ in range(config.num_layers)\n        ])\n        self.ln_f = nn.LayerNorm(config.embed_size)\n        self.fc_out = nn.Linear(config.embed_size, config.vocab_size)\n\n    def forward(self, x):\n        B, T = x.size()\n        positions = torch.arange(0, T, device=x.device).unsqueeze(0)\n        x = self.token_embedding(x) + self.position_embedding(positions)\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        x = self.fc_out(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:55.100420Z","iopub.execute_input":"2025-05-19T13:29:55.100615Z","iopub.status.idle":"2025-05-19T13:29:55.111511Z","shell.execute_reply.started":"2025-05-19T13:29:55.100599Z","shell.execute_reply":"2025-05-19T13:29:55.110993Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train(model, config):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n\n    for iter in tqdm(range(config.max_iters)):\n        x, y = get_batch(train_data, config)\n        logits = model(x)\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if iter % config.eval_interval == 0:\n            val_loss = evaluate(model, val_data, config)\n            print(f\"Iter {iter}, Train loss: {loss.item():.4f}, Val loss: {val_loss:.4f}\")\n            torch.save(model.state_dict(), config.checkpoint_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:55.112178Z","iopub.execute_input":"2025-05-19T13:29:55.112470Z","iopub.status.idle":"2025-05-19T13:29:55.127005Z","shell.execute_reply.started":"2025-05-19T13:29:55.112421Z","shell.execute_reply":"2025-05-19T13:29:55.126389Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def evaluate(model, data, config):\n    model.eval()\n    with torch.no_grad():\n        x, y = get_batch(data, config)\n        logits = model(x)\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n    model.train()\n    return loss.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:55.129055Z","iopub.execute_input":"2025-05-19T13:29:55.129576Z","iopub.status.idle":"2025-05-19T13:29:55.140665Z","shell.execute_reply.started":"2025-05-19T13:29:55.129536Z","shell.execute_reply":"2025-05-19T13:29:55.140089Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"@torch.no_grad()\ndef generate(model, start_text, max_new_tokens, config):\n    model.eval()\n    input_ids = torch.tensor(encode(start_text), dtype=torch.long)[None].to(config.device)\n    for _ in range(max_new_tokens):\n        input_cond = input_ids[:, -config.block_size:]\n        logits = model(input_cond)\n        next_id = torch.multinomial(F.softmax(logits[:, -1, :], dim=-1), num_samples=1)\n        input_ids = torch.cat((input_ids, next_id), dim=1)\n    return decode(input_ids[0].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:55.141322Z","iopub.execute_input":"2025-05-19T13:29:55.141563Z","iopub.status.idle":"2025-05-19T13:29:55.152448Z","shell.execute_reply.started":"2025-05-19T13:29:55.141525Z","shell.execute_reply":"2025-05-19T13:29:55.151935Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"config = GPTConfig(vocab_size=vocab_size)\n\nmodel = nn.DataParallel(GPT(config))\nmodel = model.to(config.device)\n\ntrain(model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:55.153147Z","iopub.execute_input":"2025-05-19T13:29:55.153371Z","iopub.status.idle":"2025-05-19T14:37:44.915258Z","shell.execute_reply.started":"2025-05-19T13:29:55.153356Z","shell.execute_reply":"2025-05-19T14:37:44.914680Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/10000 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 0, Train loss: 10.9746, Val loss: 10.8546\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 500/10000 [03:23<1:04:23,  2.46it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 500, Train loss: 5.1537, Val loss: 5.3855\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 1000/10000 [06:46<1:00:36,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 1000, Train loss: 4.5283, Val loss: 5.0002\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 1500/10000 [10:10<57:30,  2.46it/s]  ","output_type":"stream"},{"name":"stdout","text":"Iter 1500, Train loss: 4.0919, Val loss: 4.8371\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2000/10000 [13:33<53:59,  2.47it/s]  ","output_type":"stream"},{"name":"stdout","text":"Iter 2000, Train loss: 3.8362, Val loss: 4.8270\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 2500/10000 [16:56<50:40,  2.47it/s]  ","output_type":"stream"},{"name":"stdout","text":"Iter 2500, Train loss: 3.6279, Val loss: 4.4548\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3000/10000 [20:19<47:10,  2.47it/s]  ","output_type":"stream"},{"name":"stdout","text":"Iter 3000, Train loss: 3.3938, Val loss: 4.8421\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 3500/10000 [23:42<43:54,  2.47it/s]  ","output_type":"stream"},{"name":"stdout","text":"Iter 3500, Train loss: 3.1087, Val loss: 4.6405\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4000/10000 [27:06<40:38,  2.46it/s]  ","output_type":"stream"},{"name":"stdout","text":"Iter 4000, Train loss: 2.9660, Val loss: 4.8842\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 4500/10000 [30:29<37:02,  2.47it/s]  ","output_type":"stream"},{"name":"stdout","text":"Iter 4500, Train loss: 2.7121, Val loss: 4.9224\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5000/10000 [33:52<33:40,  2.47it/s]  ","output_type":"stream"},{"name":"stdout","text":"Iter 5000, Train loss: 2.3516, Val loss: 5.3779\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 5500/10000 [37:15<30:30,  2.46it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 5500, Train loss: 2.1020, Val loss: 5.2267\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6000/10000 [40:39<26:56,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 6000, Train loss: 2.0614, Val loss: 5.5510\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▌   | 6500/10000 [44:02<23:37,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 6500, Train loss: 1.7473, Val loss: 5.7895\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7000/10000 [47:25<20:13,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 7000, Train loss: 1.5266, Val loss: 6.0164\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 7500/10000 [50:48<16:47,  2.48it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 7500, Train loss: 1.3589, Val loss: 5.9709\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8000/10000 [54:12<13:29,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 8000, Train loss: 1.1969, Val loss: 6.5749\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▌ | 8500/10000 [57:35<10:08,  2.46it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 8500, Train loss: 1.0617, Val loss: 6.4508\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9000/10000 [1:00:58<06:46,  2.46it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 9000, Train loss: 0.9218, Val loss: 6.7040\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 9500/10000 [1:04:21<03:22,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"Iter 9500, Train loss: 0.8571, Val loss: 7.1811\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10000/10000 [1:07:45<00:00,  2.46it/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(generate(model, \"The King has tried\", max_new_tokens=100, config=config))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:06:47.812890Z","iopub.execute_input":"2025-05-19T15:06:47.813133Z","iopub.status.idle":"2025-05-19T15:06:48.515231Z","shell.execute_reply.started":"2025-05-19T15:06:47.813114Z","shell.execute_reply":"2025-05-19T15:06:48.514604Z"}},"outputs":[{"name":"stdout","text":"The King has tried the nose;\nLet the chaff, and butAn angry note.\n\nNurse:\nNay, he's a letter; in faith,'s a year, the case.\n\nNurse:\nUnULET:\nWill you be? half twenty cunning may!\nIt doth not Romeo, he may weep with your face?\nThe great foremost.\n\nServant:\nHave I go have it with me from his forces,\nAnd, if I\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}